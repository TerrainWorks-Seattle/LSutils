---
title: "Assessing Models"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Assessing Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params: 
  predictions: 
    label: Predictions
    value: D:/Google Drive/DataFiles/gr_mc_pca_quad_predictions/
    input: text
  training_data: 
    label: Training Data
    value: D:/Google Drive/DataFiles/gr_mc_pca_quad_predictions/train_probs.Rdata
    input: text
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(TerrainWorksUtils)
```

# Assessing Models

TerrainWorks uses a few ways of assessing models specific to the problem of landslide susceptibility. 
Some of these methods exist in the literature, such as the success rate curve, while some created by TerrainWorks. 

We define 5 goals of a measure of model performance: 
1. How well the sample of nonlandslide points characterizes the joint distributions of predictor values across the entire study area.
2. How well the chosen model algorithm characterizes the distribution of landslide densities within the data space defined by the predictors.
3. How well the choice of predictors resolves spatial variations in landslide density.
4. How sensitive model results are to the predictor values, and
5. Geomorphic plausability.

## Training data, etc. 

When assessing models, it is very important to carefully track what data is being used for what type of assessment; thiswill determine the limits on interpretation of any measure. 

Typically, a data set is divided into training and testing data sets (often multiple times, in a cross-validation scheme). Data that the model has *never* seen during the training process can be used to assess predictive performance of a model. For spatiotemporal data sets (as is the case with landslide susceptibility modeling), this split is usually along some spatial or temporal division. Since the model is guaranteed to be applied to data that is outside the temporal scope of the training data, it is often a good idea to look at temporal splits. 

For now, we are using a model that has been trained on all available landslide points, so we do not have any testing data set. 

In the case of the success rate curve, the literature differentiates curves generating using training data and testing data. When using testing data, the curve is called a prediction rate curve. 

```{r}
load(params$training_data)
train_probs <- as_tibble(as.data.frame(train_probs$prob[train_probs$truth == "pos", ]))
train_probs <- train_probs %>% rename(prob.pos = pos)
```


## The Success Rate Curve

> The “success-rate” curve was introduced by Chung and Fabbri (n.d.). To construct a success-rate curve, we rank DEM cells by the modeled probability that they contain a landslide initiation point. We then plot the proportion of mapped landslides versus the proportion of area, ranked by modeled probability.

To generate the success rate curve, we need a list of data frames that contain the modeled probabilities for each DEM that we want to consider in our assessment. 

The size of the study area, or which DEMs to include, should be carefully considered. as it will For instance, imagine if you included a completely flat portion of the study area, that had an appropriately low probability of landslide initiation. 

For this example, we pull from a small directory of DEMs. 

```{r}
preds <- as_tibble(list.files(params$predictions, "predictions.tif", 
                              full.names = TRUE,  recursive = TRUE))
preds
```

See \code{vignette(predicting_new_data)} for details on using a trained model to create predicted probabilities for multiple DEMs. 

The time-consuming part of this process is converting the rasters to data frames. 

```{r}
tic()
pred_rast <- lapply(t(preds), terra::rast)
pred_tbl <- lapply(lapply(pred_rast, as.data.frame, xy = TRUE), as_tibble)
toc()
```
There is a \code{success_rate_curve} function that calculates cumulative proportions of observed and modeled landslides, given a data frame of the predicted probabilities of observed landslides and a list of data frames containing modeled probabilities. 

An automatic plot is generated showing modeled and observed success rate curves, unless \code{plot = FALSE}. 

```{r}
s <- success_rate_curve(obs = train_probs, 
                        modeled = pred_tbl)
s
```

To help understand the success rate curve, we can add a color mapping that corresponds to the probability that maps to each point. We see that the color change in the x direction is the same for both curves, while the color change in the y direction changes. 

A single point on the success rate curve can be interpreted as the proportion of the study area that you would need to include to capture a certain proportion of the landslides. For example, the point (0.1, 0.25) would indicate that 25% of the landslides occurred in the top 10% more landslide-prone areas. 

```{r}
p <- ggplot(s) +
    geom_point(mapping = aes(x = area_prop, y = modeled_prop, color = prob),
              size = 2) +
    geom_line(mapping = aes(x = area_prop, y = observed_prop, color = prob),
              size = 2) +
    labs(title = "Success rate curve",
         x = "Proportion of area",
         y = "Proportion of landslides") +
    theme(legend.position = "right") + 
    scale_color_continuous(name = "Probability", type = "viridis")
plot(p)
```
## Proportion error

A good model will predict the same proportion of landslides as observed. A \code{calibration_bars()} function takes the result from the success rate curve function and calculates the difference of the observed proportion of landslides in 10% bins from what we expect (which should be 10%). 

The function produces the plot below, with the proportion of observed landslides found in each bin, along with a 10% reference line. 

```{r}
bars <- calibration_bars(s,
                         plot = TRUE)
```

We can also plot this as the error, or difference between that line. Here, we see that the model is over-predicting the lower modeled probabilities and under-predicting the higher modeled probabilities. 

```{r}
p <- ggplot(bars) +
        geom_col(aes(x = (prob - .05), y = observed_err)) +
        labs(x = "Modeled probability (10% bins)",
             y = "Difference in number of observed landslides") +
        scale_x_continuous(breaks = seq(0, 1, by = 0.1))

plot(p)
```
This also leads us to our single-valued measure calibration error, which is the sum of all the differences. 

```{r}
calibration_error(bars)
```

